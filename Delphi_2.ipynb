{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeqdxXtOrmpEIEJnZO0JMB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/strumer69/WebS_craping_German_jobs/blob/main/Delphi_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7usPDpY8uWo",
        "outputId": "358eb8bb-a212-4838-e4ba-a89c8d30f35b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "# Load the required libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# specifying the url , request and parsing method\n",
        "url = 'https://industrie.de/firmenverzeichnis/'\n",
        "page = requests.get(url)\n",
        "soup_html = BeautifulSoup(page.text, 'html.parser')\n",
        "\n",
        "\n",
        "# specifying the tag in which the  related urls exist\n",
        "company_name=soup_html.find_all('div' ,class_=\"infoservice-entry-holder\")\n",
        "\n",
        "\n",
        "# creating urls pool for further scraping\n",
        "urls=[]\n",
        "for i in range(0,len(company_name)):\n",
        "    a = company_name[i].find('a')\n",
        "    urls.append(a.attrs['href'])\n",
        "\n",
        "\n",
        "# main loop of program which extracts the data and saves them in csv file\n",
        "with open('companeis_data_7.csv', mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['title', 'site', 'email', 'phone','fax', 'address'])\n",
        "\n",
        "\n",
        "#-------this loop calls single url from urls and scrapes each individualy\n",
        "        for j in tqdm(range(2,len(urls))):\n",
        "\n",
        "            url=urls[j]\n",
        "            reqs = requests.get(url)\n",
        "            soup = BeautifulSoup(reqs.text, 'html')\n",
        "\n",
        "            if soup.dl:\n",
        "               dl_tag = soup.dl.extract()\n",
        "\n",
        "            else: # to handle rare urls which do not have <dl> tag\n",
        "                 dl_tag='empty'\n",
        "\n",
        "\n",
        "            # initialization of the flags and  desired data titles:\n",
        "            title=company_name[j].text\n",
        "            site=''\n",
        "            email=''\n",
        "            phone=''\n",
        "            fax=''\n",
        "            address=''\n",
        "            site_flag = 0\n",
        "            email_flag= 0\n",
        "            phone_flag= 0\n",
        "            fax_flag = 0\n",
        "            address_flag = 0\n",
        "\n",
        "\n",
        "#-----------to identify proper data title and avoid false detection due to similar tag names.\n",
        "#---------- (more explained in document challenges.text  No.3)\n",
        "            for i in dl_tag:\n",
        "\n",
        "                if dl_tag !='empty':\n",
        "                  dt_tag_string=str(i)\n",
        "\n",
        "                  if 'fa fa-globe'in dt_tag_string:\n",
        "                      site_flag = 1\n",
        "\n",
        "\n",
        "                  elif 'fa fa-envelope' in dt_tag_string:\n",
        "                      email_flag = 1\n",
        "\n",
        "                  elif'fa fa-phone' in dt_tag_string :\n",
        "                      phone_flag = 1\n",
        "\n",
        "                  elif 'fa fa-fax' in dt_tag_string:\n",
        "                      fax_flag = 1\n",
        "\n",
        "                  elif 'fa fa-map-pin' in dt_tag_string:\n",
        "                      address_flag = 1\n",
        "\n",
        "#-----------------scan <dd> tags\n",
        "                  if (site_flag==1)  & (len(i.text)!=0):\n",
        "                      site=i.text\n",
        "                      site_flag = 0\n",
        "\n",
        "                  elif (email_flag == 1) & (len(i.text) !=0):\n",
        "                      email=i.text\n",
        "                      email_flag = 0\n",
        "\n",
        "                  elif (phone_flag == 1) & (len(i.text) !=0):\n",
        "                      phone=i.text\n",
        "                      phone_flag = 0\n",
        "\n",
        "                  elif (fax_flag == 1) & (len(i.text) !=0):\n",
        "                      fax=i.text\n",
        "                      fax_flag = 0\n",
        "\n",
        "                  elif (address_flag == 1) & (len(i.text) !=0):\n",
        "                      address=i.text\n",
        "                      address_flag = 0\n",
        "                  else:\n",
        "                      pass\n",
        "\n",
        "\n",
        "            if dl_tag =='empty':\n",
        "              writer.writerow([title, '-', '-', '-','-', '-'])\n",
        "            else:\n",
        "              writer.writerow([title, site, email, phone,fax, address])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x-S0Yxt_9AJG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}